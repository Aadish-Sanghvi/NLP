{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following dataset - https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "### Apply all the preprocessing techniques that you think are necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basis Libraries    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer & stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and apply lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each review\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  one reviewer mentioned watching oz episode you...  \n",
      "1  wonderful little production filming technique ...  \n",
      "2  thought wonderful way spend time hot summer we...  \n",
      "3  basically there family little boy jake think t...  \n",
      "4  petter matteis love time money visually stunni...  \n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed dataset (optional)\n",
    "df.to_csv(\"IMDB_Cleaned_Dataset.csv\", index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(df[['review', 'cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "### Find out the number of words in the entire corpus and also the total number of unique words(vocabulary) using just python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus: 5948526\n",
      "Unique words in vocabulary: 151053\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Combine all reviews into one large text\n",
    "all_words = ' '.join(df['cleaned_review'])\n",
    "\n",
    "# Split into individual words\n",
    "word_list = all_words.split()\n",
    "\n",
    "# Count total words\n",
    "total_words = len(word_list)\n",
    "\n",
    "# Count unique words (vocabulary size), used set to remove duplicate words.\n",
    "unique_words = len(set(word_list))\n",
    "\n",
    "print(f\"Total words in corpus: {total_words}\")\n",
    "print(f\"Unique words in vocabulary: {unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "### Apply One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying one-hot encoding directly to the entire IMDB dataset is memory-intensive because:\n",
    "\n",
    "âœ… The dataset has 50,000 reviews.\n",
    "\n",
    "âœ… The vocabulary size is huge (likely 50,000+ unique words).\n",
    "\n",
    "âœ… Each word creates a new column, leading to an enormous sparse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "### Apply bag words and find the vocabulary also find the times each word has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 151029\n",
      "Top 10 Most Frequent Words:\n",
      "movie        100957\n",
      "film          91574\n",
      "one           53811\n",
      "like          40022\n",
      "time          30249\n",
      "good          29030\n",
      "character     27982\n",
      "story         24740\n",
      "even          24587\n",
      "get           24517\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Limit vocabulary to 5000 most common words\n",
    "vectorizer = CountVectorizer(dtype=np.int8)\n",
    "\n",
    "# Fit and transform the cleaned reviews\n",
    "bow_matrix = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get word frequencies\n",
    "word_counts = bow_df.sum().sort_values(ascending=False)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(\"Top 10 Most Frequent Words:\")\n",
    "print(word_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "### Apply bag of bi-gram and bag of tri-gram and write down your observation about the dimensionality of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (Bi-gram): 2992555\n",
      "Vocabulary Size (Tri-gram): 5366114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Bi-gram Model (2-word sequences)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), dtype=np.int8)\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(df['cleaned_review'])\n",
    "bigram_vocab_size = len(bigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Tri-gram Model (3-word sequences)\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3), dtype=np.int8)\n",
    "trigram_matrix = trigram_vectorizer.fit_transform(df['cleaned_review'])\n",
    "trigram_vocab_size = len(trigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(f\"Vocabulary Size (Bi-gram): {bigram_vocab_size}\")\n",
    "print(f\"Vocabulary Size (Tri-gram): {trigram_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Higher n-grams increase the feature space â†’ more memory usage â†’ higher computation cost.\n",
    "\n",
    "ðŸ“Œ Capturing word sequences improves context understanding but also leads to sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "### Apply tf-idf and find out the idf scores of words, also find out the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Give importance to words that appear frequently in a document but are rare in the dataset.\n",
    "\n",
    "âœ… Reduce the effect of common words (like \"the\", \"is\", etc.).\n",
    "\n",
    "âœ… Balance word frequency with uniqueness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 151029\n",
      "Top 10 Most Unique Words Based on IDF Scores:\n",
      "               Word  IDF_Score\n",
      "75514        lifeha  11.126651\n",
      "87720    mukerjhees  11.126651\n",
      "87745      mulgrews  11.126651\n",
      "87743       mulford  11.126651\n",
      "87742      mulelike  11.126651\n",
      "87740       muldrun  11.126651\n",
      "87738  mulderscully  11.126651\n",
      "87735       muldayr  11.126651\n",
      "87734    muldaurwho  11.126651\n",
      "87732        muldar  11.126651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned reviews\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Get the vocabulary (all unique words)\n",
    "vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Vocabulary Size: {len(vocabulary)}\")\n",
    "\n",
    "# Get the IDF scores for each word\n",
    "idf_scores = dict(zip(vocabulary, tfidf_vectorizer.idf_))\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "idf_df = pd.DataFrame(idf_scores.items(), columns=['Word', 'IDF_Score']).sort_values(by=\"IDF_Score\", ascending=False)\n",
    "\n",
    "# Display top 10 words with highest IDF scores (most unique words)\n",
    "print(\"Top 10 Most Unique Words Based on IDF Scores:\")\n",
    "print(idf_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
